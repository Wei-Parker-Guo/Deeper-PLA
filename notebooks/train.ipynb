{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5daee96-7c75-4a43-a825-c2ea707e8e38",
   "metadata": {},
   "source": [
    "# Training the Model\n",
    "In this notebook I will train the derived model (AssemblyModel) to predict protein-ligand affinity. I choose to do this in a notebook because results are easier to track and manipulate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6eac1137-eb13-4c16-8a9f-d0a2911ef33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports, solves custom package importing by appending cwd to system paths\n",
    "import os, sys\n",
    "parent_dir = os.path.abspath('..')\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from src.models.assembly_model import AssemblyModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1de8ccba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running threads on 16 cpu cores.\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "K_CORES = multiprocessing.cpu_count()\n",
    "print(\"Running threads on {} cpu cores.\".format(K_CORES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9dda920-9eeb-4afd-81c7-b374836a6d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected GPU NVIDIA GeForce RTX 3080, training will run on it.\n"
     ]
    }
   ],
   "source": [
    "# use gpu if it's available\n",
    "DEVICE = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    print('Detected GPU {}, training will run on it.'.format(torch.cuda.get_device_name(0)))\n",
    "    DEVICE = 'cuda'\n",
    "else:\n",
    "    print('No GPUs available, will run on cpu.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "142f1bda-7c73-4b8a-8a8c-0d7cfcff4cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# glboal vars controlling training logic\n",
    "GEN_DATA = False  # will not generate the training data again on disk\n",
    "# directories to save the generated data in and how we divide the data\n",
    "train_save_dir = '../data/generated/train'\n",
    "valid_save_dir = '../data/generated/valid'\n",
    "train_ratio = 0.8\n",
    "chunk_size = 30  # we divide data into chunks to save on disk\n",
    "total_pairs_n = 3000  # total number of pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c666471b-21c6-474d-90c2-8c88e54ec07a",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "We prepare the training data and functions in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e13393b-ff97-498a-a637-7a550bfccdd6",
   "metadata": {},
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94c00561-9305-4f54-8350-c3e57bd5035f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We choose to have a global preprocess here to save memory.\n",
    "# The train/test datasets we are gonna later will simply query it.\n",
    "from src.preprocess.preprocess import TrainPreprocessor\n",
    "\n",
    "if GEN_DATA:\n",
    "    train_processor = TrainPreprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8e3eac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as the datasets takes a long time to generate (because of the loops in voxelization)\n",
    "# we cache them on disk in separate batches.\n",
    "# we accelerate with multi threads\n",
    "from tqdm.notebook import tqdm\n",
    "from multiprocessing import Process\n",
    "from src.gen_dataset import gen_dataset\n",
    "\n",
    "def gen_dataset_threaded(pairs, cache_dir, rot_aug=True, batch_size=2, chunk_size=30, cache_on_disk=True, k_core=4):\n",
    "    with tqdm(total=len(pairs)) as pbar:\n",
    "        for i in range(0, len(pairs), chunk_size * k_core):\n",
    "            ts = []\n",
    "            for ti in range(k_core):\n",
    "                start = i + chunk_size * ti\n",
    "                if start > len(pairs):\n",
    "                    break\n",
    "                end = start + chunk_size\n",
    "                end = end if end < len(pairs) else len(pairs)\n",
    "                ts.append(Process(target=gen_dataset, \n",
    "                                  args=(pairs[start:end], cache_dir, \n",
    "                                        start//chunk_size, train_processor, \n",
    "                                        rot_aug, batch_size, cache_on_disk)))\n",
    "                ts[-1].start()\n",
    "            for t in ts:\n",
    "                t.join()\n",
    "            pbar.write('Processed {} pairs.'.format(i + chunk_size * k_core))\n",
    "            pbar.update(chunk_size * k_core)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefc2201-ef16-49f8-ad26-2caeebf06163",
   "metadata": {},
   "source": [
    "Only run this cell if you want to generate the data again. It will take a long time on cpu!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b89e1f1-7f80-435f-b34b-fc83fd155c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# generate train\n",
    "if __name__=='__main__':\n",
    "    if GEN_DATA:\n",
    "        # generate 80% train and 20% validation\n",
    "        pairs = list(train_processor.gt_pairs.items())\n",
    "        pairs_train = random.sample(pairs, int(len(pairs) * train_ratio))\n",
    "        pairs_valid = [x for x in pairs if x not in pairs_train]\n",
    "        \n",
    "        gen_dataset_threaded(pairs_train, train_save_dir, k_core=K_CORES-1)\n",
    "        gen_dataset_threaded(pairs_valid, valid_save_dir, k_core=K_CORES-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfa6ef5-5ff6-455e-8545-1a8e2d73d902",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d7f4da-922c-473c-a463-caae0f07c0e4",
   "metadata": {},
   "source": [
    "### Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9148c777-8394-4010-8a11-60c494f84051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for training loss, we combine l1 and l2 with weights\n",
    "def train_loss(pred, target):\n",
    "    eps = 1e-5\n",
    "    pred = pred.to('cpu')\n",
    "    target = target.reshape(-1, 1).expand(-1, pred.shape[1])\n",
    "    loss = -target * torch.log(pred + eps) - (1 - target) * torch.log(1 - pred + eps)\n",
    "    loss = torch.mean(loss, dim=1)\n",
    "    loss = torch.sum(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec3ae3ef-7d54-49f4-aa40-da01a25ceff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_loss(pred, target):\n",
    "    with torch.no_grad():\n",
    "        eps = 1e-5\n",
    "        pred = pred.to('cpu')\n",
    "        loss = -torch.mean(target * torch.log(pred + eps) + (1 - target) * torch.log(1 - pred + eps))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71dc0f40-e814-4175-94dd-dc7e491cc411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the training function\n",
    "def train_one_epoch(epoch_index, batch_rp_size, tb_writer, loader, model, optimizer, loss_fn):\n",
    "    running_loss = 0.0\n",
    "    last_loss = 0.0\n",
    "    \n",
    "    for batch, X in enumerate(loader):\n",
    "        x1 = X[0]  # grid\n",
    "        x2 = X[1]  # embeds\n",
    "        target = X[2]  # labels\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute prediction and loss\n",
    "        pred = model(x1, x2)\n",
    "        loss = loss_fn(pred.float(), target.float())\n",
    "        \n",
    "        # Back Propagate\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Gather data and report to tensorboard\n",
    "        running_loss += loss.item()\n",
    "        if batch % batch_rp_size == batch_rp_size - 1:\n",
    "            last_loss = running_loss / batch_rp_size # loss per batch\n",
    "            tb_x = epoch_index * len(loader) + batch + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.0\n",
    "        \n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ade3385-67b9-4129-9b63-7261328c16a3",
   "metadata": {},
   "source": [
    "## Training\n",
    "With everything set, we can start to train the model now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12caff61-66b4-40c1-bb57-1f407c26f2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.assembly_model import AssemblyModel\n",
    "\n",
    "# training hyperparams\n",
    "model_save_dir = '../models'\n",
    "tb_save_dir = '../models/runs'\n",
    "\n",
    "TOTAL_EPOCHS = 60  # the total epochs we use to go through the entire dataset\n",
    "epoch_number = 0\n",
    "best_vloss = 1_000_000\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "early_stop_t = 0.1  # threshold for eTOTAL_EPOCHStopping\n",
    "\n",
    "# create model\n",
    "model = AssemblyModel(device=DEVICE)\n",
    "\n",
    "# optimisation\n",
    "loss_fn = train_loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=4e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "470c2f00-09fb-4541-8b81-ef3bba5f49fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('{}/train_{}'.format(tb_save_dir, timestamp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cccdde-1be8-4cb1-8fc9-a83ec4df1348",
   "metadata": {},
   "source": [
    "### Training Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85d54728-8b84-478c-8962-f740d65d2000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80ad6cd6232240acbbf128cc7f6e8cb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "LOSS train 23.828877449035645 valid 0.7345786271577405\n",
      "EPOCH 2:\n",
      "LOSS train 25.104795932769775 valid 1.2149989417820226\n",
      "EPOCH 3:\n",
      "LOSS train 24.717485904693604 valid 0.7343309702023758\n",
      "EPOCH 4:\n",
      "LOSS train 26.88937282562256 valid 0.7317704448323591\n",
      "EPOCH 5:\n",
      "LOSS train 26.84062623977661 valid 0.7121472005261845\n",
      "EPOCH 6:\n",
      "LOSS train 26.15716314315796 valid 0.7581125077826769\n",
      "EPOCH 7:\n",
      "LOSS train 26.850772857666016 valid 0.8837772757607318\n",
      "EPOCH 8:\n",
      "LOSS train 27.981881618499756 valid 1.078766767382019\n",
      "EPOCH 9:\n",
      "LOSS train 27.311137199401855 valid 0.7792108246650352\n",
      "EPOCH 10:\n",
      "LOSS train 23.091619968414307 valid 0.7864697836312861\n",
      "EPOCH 11:\n",
      "LOSS train 24.98884391784668 valid 0.8572819390791765\n",
      "EPOCH 12:\n",
      "LOSS train 24.19709825515747 valid 0.7719017570202037\n",
      "EPOCH 13:\n",
      "LOSS train 30.18438959121704 valid 0.8641032175451229\n",
      "EPOCH 14:\n",
      "LOSS train 27.71218490600586 valid 0.8412620658434745\n",
      "EPOCH 15:\n",
      "LOSS train 25.20663356781006 valid 0.817432746189867\n",
      "EPOCH 16:\n",
      "LOSS train 24.960410594940186 valid 0.8232001996759731\n",
      "EPOCH 17:\n",
      "LOSS train 24.01025152206421 valid 0.7829776320968959\n",
      "EPOCH 18:\n",
      "LOSS train 24.91906213760376 valid 0.8115462030794598\n",
      "EPOCH 19:\n",
      "LOSS train 24.05222177505493 valid 0.8579063142832187\n",
      "EPOCH 20:\n",
      "LOSS train 25.473853588104248 valid 1.6201677390370308\n",
      "EPOCH 21:\n",
      "LOSS train 26.73723793029785 valid 0.6947449485742254\n",
      "EPOCH 22:\n",
      "LOSS train 24.055728435516357 valid 0.7380005352778136\n",
      "EPOCH 23:\n",
      "LOSS train 23.884282112121582 valid 0.8492531618377633\n",
      "EPOCH 24:\n",
      "LOSS train 29.295331954956055 valid 1.0497482683344954\n",
      "EPOCH 25:\n",
      "LOSS train 25.075660228729248 valid 0.9244963299970473\n",
      "EPOCH 26:\n",
      "LOSS train 25.597711086273193 valid 0.9114508630313031\n",
      "EPOCH 27:\n",
      "LOSS train 26.036481857299805 valid 0.8374473296528757\n",
      "EPOCH 28:\n",
      "LOSS train 28.43935251235962 valid 0.8369288015412587\n",
      "EPOCH 29:\n",
      "LOSS train 27.343876361846924 valid 0.9417892774522686\n",
      "EPOCH 30:\n",
      "LOSS train 26.440448760986328 valid 1.2807121745109769\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_30072/3212022639.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtds\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m                 \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}/{}.data'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_save_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtds_indices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m                 \u001b[0mtrain_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\environment\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    605\u001b[0m                     \u001b[0mopened_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morig_position\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    606\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 607\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    608\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\environment\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[0;32m    880\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mUnpicklerWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    881\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 882\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\environment\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m    855\u001b[0m         \u001b[0mdata_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    856\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 857\u001b[1;33m             \u001b[0mload_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    858\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    859\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mstorage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\environment\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[1;34m(data_type, size, key, location)\u001b[0m\n\u001b[0;32m    843\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    844\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 845\u001b[1;33m         \u001b[0mstorage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_storage_from_record\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    846\u001b[0m         \u001b[0mloaded_storages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrestore_location\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    847\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# as training data is too large, we try to dynamically load it in chunks from disk as we train\n",
    "from src.gen_dataset import ProteinLigandDataset\n",
    "\n",
    "\n",
    "with tqdm(total=TOTAL_EPOCHS * 10) as pbar:\n",
    "    for i in range(TOTAL_EPOCHS):\n",
    "\n",
    "        tds_indices = list(range(80))\n",
    "        vds_indices = list(range(20))\n",
    "        random.shuffle(tds_indices)\n",
    "        random.shuffle(vds_indices)\n",
    "\n",
    "        for ds in range(0, 20, 2):\n",
    "            tds = ds * 4\n",
    "            # load generated data from disk\n",
    "            train_set = ProteinLigandDataset([], None, 0, rot_aug=True)\n",
    "            valid_set = ProteinLigandDataset([], None, 0, rot_aug=True)\n",
    "\n",
    "            for i in range(tds, tds + 8):\n",
    "                s = torch.load('{}/{}.data'.format(train_save_dir, tds_indices[i]))\n",
    "                train_set.concat(s)\n",
    "\n",
    "            for i in range(ds, ds + 2):\n",
    "                s = torch.load('{}/{}.data'.format(valid_save_dir, vds_indices[i]))\n",
    "                valid_set.concat(s)\n",
    "\n",
    "            train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=K_CORES//2)\n",
    "            valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=K_CORES//2)\n",
    "\n",
    "            pbar.write('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "            # Make sure gradient tracking is on, and do a pass over the data\n",
    "            model.train(True)\n",
    "            avg_loss = train_one_epoch(epoch_number, 4, writer, train_loader, model, optimizer, loss_fn)\n",
    "\n",
    "\n",
    "            # We don't need gradients on to do reporting\n",
    "            model.train(False)\n",
    "\n",
    "            running_vloss = 0.0\n",
    "            for i, vdata in enumerate(valid_loader):\n",
    "                vx1, vx2, vlabels = vdata\n",
    "                voutputs = model(vx1, vx2)\n",
    "                vloss = valid_loss(voutputs, vlabels)\n",
    "                running_vloss += vloss\n",
    "\n",
    "            avg_vloss = running_vloss / (i + 1)\n",
    "            pbar.write('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "            # Log the running loss averaged per batch\n",
    "            # for both training and validation\n",
    "            writer.add_scalars('Training vs. Validation Loss',\n",
    "                            { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                            epoch_number + 1)\n",
    "            writer.flush()\n",
    "\n",
    "            # Track best performance, and save the model's state\n",
    "            if avg_vloss < best_vloss:\n",
    "                best_vloss = avg_vloss\n",
    "                model_path = '{}/model_{}_{}'.format(model_save_dir, timestamp, epoch_number)\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "\n",
    "            epoch_number += 1\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937d9801-9ad3-4326-90c3-4c48bc9af4f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Example Predicting Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fb4fc80-e8cf-4f80-9ebf-e08180c715fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.gen_dataset import ProteinLigandDataset\n",
    "\n",
    "model_path = '../models/model_20220228_223914_20'\n",
    "model = AssemblyModel(device=DEVICE)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "train_set = ProteinLigandDataset([], None, 0, rot_aug=True)\n",
    "s = torch.load('{}/{}.data'.format(train_save_dir, 1))\n",
    "train_set.concat(s)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=4, shuffle=True, num_workers=K_CORES//2)\n",
    "\n",
    "preds = []\n",
    "labels = []\n",
    "\n",
    "for i, X in enumerate(train_loader):\n",
    "    x1, x2, label = X\n",
    "    model.train(False)\n",
    "    pred = model(x1, x2)\n",
    "    preds.append(pred)\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "959e2c98-d0e2-49cd-9891-c3bea4b15320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[0.5165],\n",
      "        [0.5179],\n",
      "        [0.5050],\n",
      "        [0.5156]], device='cuda:0', grad_fn=<SigmoidBackward0>), tensor([[0.5239],\n",
      "        [0.5128],\n",
      "        [0.5833],\n",
      "        [0.5146]], device='cuda:0', grad_fn=<SigmoidBackward0>), tensor([[0.5136],\n",
      "        [0.5146],\n",
      "        [0.5169],\n",
      "        [0.5085]], device='cuda:0', grad_fn=<SigmoidBackward0>), tensor([[0.8928],\n",
      "        [0.6253],\n",
      "        [0.5193],\n",
      "        [0.5146]], device='cuda:0', grad_fn=<SigmoidBackward0>), tensor([[0.5193],\n",
      "        [0.5083],\n",
      "        [0.5141],\n",
      "        [0.6421]], device='cuda:0', grad_fn=<SigmoidBackward0>)]\n"
     ]
    }
   ],
   "source": [
    "print(preds[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd8e858d-6f93-42d6-b2c6-5f98d96797db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([0., 0., 0., 1.], dtype=torch.float64), tensor([0., 1., 0., 1.], dtype=torch.float64), tensor([1., 1., 0., 1.], dtype=torch.float64), tensor([0., 1., 0., 1.], dtype=torch.float64), tensor([0., 1., 1., 0.], dtype=torch.float64)]\n"
     ]
    }
   ],
   "source": [
    "print(labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3668b22-971b-47ab-b790-fde82e647337",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
