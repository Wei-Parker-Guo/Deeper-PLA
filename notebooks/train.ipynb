{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5daee96-7c75-4a43-a825-c2ea707e8e38",
   "metadata": {},
   "source": [
    "# Training the Model\n",
    "In this notebook I will train the derived model (AssemblyModel) to predict protein-ligand affinity. I choose to do this in a notebook because results are easier to track and manipulate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eac1137-eb13-4c16-8a9f-d0a2911ef33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports, solves custom package importing by appending cwd to system paths\n",
    "import os, sys\n",
    "parent_dir = os.path.abspath('..')\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from src.models.assembly_model import AssemblyModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dda920-9eeb-4afd-81c7-b374836a6d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use gpu if it's available\n",
    "DEVICE = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    print('Detected GPU {}, training will run on it.'.format(torch.cuda.get_device_name(0)))\n",
    "    DEVICE = 'cuda'\n",
    "else:\n",
    "    print('No GPUs available, will run on cpu.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c666471b-21c6-474d-90c2-8c88e54ec07a",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "We prepare the training data and functions in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e13393b-ff97-498a-a637-7a550bfccdd6",
   "metadata": {},
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c00561-9305-4f54-8350-c3e57bd5035f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We choose to have a global preprocess here to save memory.\n",
    "# The train/test datasets we are gonna later will simply query it.\n",
    "from src.preprocess.preprocess import TrainPreprocessor\n",
    "\n",
    "train_processor = TrainPreprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55347ae-c790-44f8-a5f3-976bde961a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a custom dataset format for our data\n",
    "import random\n",
    "import pandas as pd\n",
    "from typing import NamedTuple\n",
    "from torch.utils.data import Dataset\n",
    "from src.preprocess.bind_grid import BindGrid\n",
    "from src.global_vars import AUGMENT_ROTATION\n",
    "\n",
    "\n",
    "class DataItem(NamedTuple):\n",
    "    grid: torch.Tensor\n",
    "    embed: torch.Tensor\n",
    "    label: float\n",
    "\n",
    "class ProteinLigandDataset(Dataset):\n",
    "    def __init__(self, pairs, batch_size, rot_aug=True):\n",
    "        self.pairs = pairs  # a list of pairs from querying pair.csv\n",
    "        self.batch_size = batch_size\n",
    "        self.rot_aug = rot_aug  # augment data by random grid rotation\n",
    "        self.data = []\n",
    "        self.gen_data()\n",
    "    \n",
    "    # based on the given pairs, generate the full dataset with both true and false pairs\n",
    "    def gen_data(self):\n",
    "        smiles = []\n",
    "        grids = []\n",
    "        labels = []\n",
    "        for pair in self.pairs:\n",
    "            pid, lid = pair\n",
    "            for i in range(self.batch_size):\n",
    "                lid = int(lid)\n",
    "                if i != 0:  # generate false pair\n",
    "                    prev_lid = lid\n",
    "                    while prev_lid == lid or (str(lid) not in train_processor.gt_pairs.values()):\n",
    "                        lid = random.randint(1, len(train_processor.ligands))\n",
    "                lid = str(lid)\n",
    "                grid = BindGrid(train_processor.proteins[pid], train_processor.ligands[lid], train_processor.centroids[pid])\n",
    "                gs = [grid.grid]\n",
    "                smiles.append(train_processor.ligands[lid].smiles)\n",
    "                if self.rot_aug:\n",
    "                    gs = grid.rotation_augment()\n",
    "                for j in range(len(gs)):\n",
    "                    grids.append(gs[j])\n",
    "                    if i == 0:\n",
    "                        labels.append(1.0)\n",
    "                    else:\n",
    "                        labels.append(0.0)\n",
    "        \n",
    "        embeds = train_processor.generate_embeddings(smiles)\n",
    "        for i in range(len(labels)):\n",
    "            self.data.append(DataItem(grids[i], embeds[i//(AUGMENT_ROTATION+1)], labels[i]))\n",
    "            \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        i = self.data[idx]\n",
    "        return i.grid, i.embed, i.label\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163d4c6f-4a18-4d76-b3b2-187c98d82d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as the datasets takes a long time to generate (because of the loops in voxelization)\n",
    "# we cache them on disk in separate batches.\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def gen_dataset(pairs, cache_dir, rot_aug=True, batch_size=2, chunk_size=30, cache_on_disk=True):\n",
    "    with tqdm(total=len(pairs)) as pbar:\n",
    "        for i in range(0, len(pairs), chunk_size):\n",
    "            e = i + chunk_size if i + chunk_size <= len(pairs) else len(pairs)\n",
    "            dataset = ProteinLigandDataset(pairs[i:e], batch_size, rot_aug)\n",
    "            torch.save(dataset, '{}/{}.data'.format(cache_dir, i))\n",
    "            pbar.write('Processed {} pairs.'.format(e-i))\n",
    "            pbar.update(e-i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32762b0-766f-4173-a5da-edd87e0d657a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 80% train and 20% validation\n",
    "train_save_dir = '../data/generated/train'\n",
    "valid_save_dir = '../data/generated/valid'\n",
    "\n",
    "pairs = list(train_processor.gt_pairs.items())\n",
    "pairs_train = random.sample(pairs, int(len(pairs) * 0.8))\n",
    "pairs_valid = [x for x in pairs if x not in pairs_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefc2201-ef16-49f8-ad26-2caeebf06163",
   "metadata": {},
   "source": [
    "Only run these two cells if you want to generate the data again.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b89e1f1-7f80-435f-b34b-fc83fd155c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate train\n",
    "gen_dataset(pairs_train, train_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9f5448-4d76-40de-8382-3b466ad8e018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate valid\n",
    "gen_dataset(pairs_valid, valid_save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfa6ef5-5ff6-455e-8545-1a8e2d73d902",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9c2e47-c800-4aab-91c9-ef5540c22648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the data from the pair ids given, returns a dataloader\n",
    "# a pair id is just the index of that pair in pairs.csv\n",
    "def prepare_data(batch_size, pair_ids):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d7f4da-922c-473c-a463-caae0f07c0e4",
   "metadata": {},
   "source": [
    "### Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024e4946-4cb9-4264-90f4-aa3214ed7406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the train loss accumulates loss from each 1024 vector\n",
    "def train_loss(pred, target, loss_fn):\n",
    "    target = target.expand(-1, pred.size()[1])\n",
    "    loss = loss_fn(pred, target)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dc0f40-e814-4175-94dd-dc7e491cc411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the training function\n",
    "def train(loader, model, loss_fn, optimizer, lapse):\n",
    "    for batch, (X, y) in enumerate(loader):\n",
    "        x1 = X[0]\n",
    "        x2 = X[1]\n",
    "        # Compute prediction and loss\n",
    "        pred = model(x1, x2)\n",
    "        loss = train_loss(pred, target, loss_fn)\n",
    "        \n",
    "        # Back Propagate\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % lapse == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
