{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5daee96-7c75-4a43-a825-c2ea707e8e38",
   "metadata": {},
   "source": [
    "# Training the Model\n",
    "In this notebook I will train the derived model (AssemblyModel) to predict protein-ligand affinity. I choose to do this in a notebook because results are easier to track and manipulate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6eac1137-eb13-4c16-8a9f-d0a2911ef33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports, solves custom package importing by appending cwd to system paths\n",
    "import os, sys\n",
    "parent_dir = os.path.abspath('..')\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from src.models.assembly_model import AssemblyModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1de8ccba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running threads on 16 cpu cores.\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "K_CORES = multiprocessing.cpu_count()\n",
    "print(\"Running threads on {} cpu cores.\".format(K_CORES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9dda920-9eeb-4afd-81c7-b374836a6d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected GPU NVIDIA GeForce RTX 3080, training will run on it.\n"
     ]
    }
   ],
   "source": [
    "# use gpu if it's available\n",
    "DEVICE = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    print('Detected GPU {}, training will run on it.'.format(torch.cuda.get_device_name(0)))\n",
    "    DEVICE = 'cuda'\n",
    "else:\n",
    "    print('No GPUs available, will run on cpu.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c666471b-21c6-474d-90c2-8c88e54ec07a",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "We prepare the training data and functions in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e13393b-ff97-498a-a637-7a550bfccdd6",
   "metadata": {},
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94c00561-9305-4f54-8350-c3e57bd5035f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We choose to have a global preprocess here to save memory.\n",
    "# The train/test datasets we are gonna later will simply query it.\n",
    "from src.preprocess.preprocess import TrainPreprocessor\n",
    "\n",
    "train_processor = TrainPreprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8e3eac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as the datasets takes a long time to generate (because of the loops in voxelization)\n",
    "# we cache them on disk in separate batches.\n",
    "# we accelerate with multi threads\n",
    "from tqdm.notebook import tqdm\n",
    "from multiprocessing import Process\n",
    "from src.gen_dataset import gen_dataset\n",
    "\n",
    "def gen_dataset_threaded(pairs, cache_dir, rot_aug=True, batch_size=2, chunk_size=30, cache_on_disk=True, k_core=4):\n",
    "    with tqdm(total=len(pairs)) as pbar:\n",
    "        for i in range(0, len(pairs), chunk_size * k_core):\n",
    "            ts = []\n",
    "            for ti in range(k_core):\n",
    "                start = i + chunk_size * ti\n",
    "                if start > len(pairs):\n",
    "                    break\n",
    "                end = start + chunk_size\n",
    "                end = end if end < len(pairs) else len(pairs)\n",
    "                ts.append(Process(target=gen_dataset, \n",
    "                                  args=(pairs[start:end], cache_dir, \n",
    "                                        start//chunk_size, train_processor, \n",
    "                                        rot_aug, batch_size, cache_on_disk)))\n",
    "                ts[-1].start()\n",
    "            for t in ts:\n",
    "                t.join()\n",
    "            pbar.write('Processed {} pairs.'.format(i + chunk_size * k_core))\n",
    "            pbar.update(chunk_size * k_core)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c32762b0-766f-4173-a5da-edd87e0d657a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 80% train and 20% validation\n",
    "import random\n",
    "\n",
    "train_save_dir = '../data/generated/train'\n",
    "valid_save_dir = '../data/generated/valid'\n",
    "\n",
    "pairs = list(train_processor.gt_pairs.items())\n",
    "pairs_train = random.sample(pairs, int(len(pairs) * 0.8))\n",
    "pairs_valid = [x for x in pairs if x not in pairs_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefc2201-ef16-49f8-ad26-2caeebf06163",
   "metadata": {},
   "source": [
    "Only run these two cells if you want to generate the data again.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b89e1f1-7f80-435f-b34b-fc83fd155c86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9212e6b0cbe04210a4c96301414a7ff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 450 pairs.\n",
      "Processed 900 pairs.\n",
      "Processed 1350 pairs.\n",
      "Processed 1800 pairs.\n",
      "Processed 2250 pairs.\n",
      "Processed 2700 pairs.\n"
     ]
    }
   ],
   "source": [
    "# generate train\n",
    "if __name__=='__main__':\n",
    "    gen_dataset_threaded(pairs_train, train_save_dir, k_core=K_CORES-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf9f5448-4d76-40de-8382-3b466ad8e018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6beb30c7ccde41b384e68e652f57dbd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 450 pairs.\n",
      "Processed 900 pairs.\n"
     ]
    }
   ],
   "source": [
    "# generate valid\n",
    "if __name__=='__main__':\n",
    "    gen_dataset_threaded(pairs_valid, valid_save_dir, k_core=K_CORES-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfa6ef5-5ff6-455e-8545-1a8e2d73d902",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9c2e47-c800-4aab-91c9-ef5540c22648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the data from the pair ids given, returns a dataloader\n",
    "# a pair id is just the index of that pair in pairs.csv\n",
    "def prepare_data(batch_size, pair_ids):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d7f4da-922c-473c-a463-caae0f07c0e4",
   "metadata": {},
   "source": [
    "### Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024e4946-4cb9-4264-90f4-aa3214ed7406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the train loss accumulates loss from each 1024 vector\n",
    "def train_loss(pred, target, loss_fn):\n",
    "    target = target.expand(-1, pred.size()[1])\n",
    "    loss = loss_fn(pred, target)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dc0f40-e814-4175-94dd-dc7e491cc411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the training function\n",
    "def train(loader, model, loss_fn, optimizer, lapse):\n",
    "    for batch, (X, y) in enumerate(loader):\n",
    "        x1 = X[0]\n",
    "        x2 = X[1]\n",
    "        # Compute prediction and loss\n",
    "        pred = model(x1, x2)\n",
    "        loss = train_loss(pred, target, loss_fn)\n",
    "        \n",
    "        # Back Propagate\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % lapse == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
