{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5daee96-7c75-4a43-a825-c2ea707e8e38",
   "metadata": {},
   "source": [
    "# Training the Model\n",
    "In this notebook I will train the derived model (AssemblyModel) to predict protein-ligand affinity. I choose to do this in a notebook because results are easier to track and manipulate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6eac1137-eb13-4c16-8a9f-d0a2911ef33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports, solves custom package importing by appending cwd to system paths\n",
    "import os, sys\n",
    "parent_dir = os.path.abspath('..')\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from src.models.assembly_model import AssemblyModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1de8ccba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running threads on 16 cpu cores.\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "K_CORES = multiprocessing.cpu_count()\n",
    "print(\"Running threads on {} cpu cores.\".format(K_CORES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9dda920-9eeb-4afd-81c7-b374836a6d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPUs available, will run on cpu.\n"
     ]
    }
   ],
   "source": [
    "# use gpu if it's available\n",
    "DEVICE = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    print('Detected GPU {}, training will run on it.'.format(torch.cuda.get_device_name(0)))\n",
    "    DEVICE = 'cuda'\n",
    "else:\n",
    "    print('No GPUs available, will run on cpu.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "142f1bda-7c73-4b8a-8a8c-0d7cfcff4cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# glboal vars controlling training logic\n",
    "GEN_DATA = False  # will not generate the training data again on disk\n",
    "# directories to save the generated data in and how we divide the data\n",
    "train_save_dir = '../data/generated/train'\n",
    "valid_save_dir = '../data/generated/valid'\n",
    "train_ratio = 0.8\n",
    "chunk_size = 30  # we divide data into chunks to save on disk\n",
    "total_pairs_n = 3000  # total number of pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c666471b-21c6-474d-90c2-8c88e54ec07a",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "We prepare the training data and functions in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e13393b-ff97-498a-a637-7a550bfccdd6",
   "metadata": {},
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94c00561-9305-4f54-8350-c3e57bd5035f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We choose to have a global preprocess here to save memory.\n",
    "# The train/test datasets we are gonna later will simply query it.\n",
    "from src.preprocess.preprocess import TrainPreprocessor\n",
    "\n",
    "if GEN_DATA:\n",
    "    train_processor = TrainPreprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8e3eac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as the datasets takes a long time to generate (because of the loops in voxelization)\n",
    "# we cache them on disk in separate batches.\n",
    "# we accelerate with multi threads\n",
    "from tqdm.notebook import tqdm\n",
    "from multiprocessing import Process\n",
    "from src.gen_dataset import gen_dataset\n",
    "\n",
    "def gen_dataset_threaded(pairs, cache_dir, rot_aug=True, batch_size=2, chunk_size=30, cache_on_disk=True, k_core=4):\n",
    "    with tqdm(total=len(pairs)) as pbar:\n",
    "        for i in range(0, len(pairs), chunk_size * k_core):\n",
    "            ts = []\n",
    "            for ti in range(k_core):\n",
    "                start = i + chunk_size * ti\n",
    "                if start > len(pairs):\n",
    "                    break\n",
    "                end = start + chunk_size\n",
    "                end = end if end < len(pairs) else len(pairs)\n",
    "                ts.append(Process(target=gen_dataset, \n",
    "                                  args=(pairs[start:end], cache_dir, \n",
    "                                        start//chunk_size, train_processor, \n",
    "                                        rot_aug, batch_size, cache_on_disk)))\n",
    "                ts[-1].start()\n",
    "            for t in ts:\n",
    "                t.join()\n",
    "            pbar.write('Processed {} pairs.'.format(i + chunk_size * k_core))\n",
    "            pbar.update(chunk_size * k_core)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefc2201-ef16-49f8-ad26-2caeebf06163",
   "metadata": {},
   "source": [
    "Only run this cell if you want to generate the data again. It will take a long time on cpu!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b89e1f1-7f80-435f-b34b-fc83fd155c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# generate train\n",
    "if __name__=='__main__':\n",
    "    if GEN_DATA:\n",
    "        # generate 80% train and 20% validation\n",
    "        pairs = list(train_processor.gt_pairs.items())\n",
    "        pairs_train = random.sample(pairs, int(len(pairs) * train_ratio))\n",
    "        pairs_valid = [x for x in pairs if x not in pairs_train]\n",
    "        \n",
    "        gen_dataset_threaded(pairs_train, train_save_dir, k_core=K_CORES-1)\n",
    "        gen_dataset_threaded(pairs_valid, valid_save_dir, k_core=K_CORES-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfa6ef5-5ff6-455e-8545-1a8e2d73d902",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e9c2e47-c800-4aab-91c9-ef5540c22648",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 3 required positional arguments: 'pairs', 'train_processor', and 'batch_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ky/gg6ncd7j14sc1m2sf0yhgxmc0000gn/T/ipykernel_1423/633498501.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen_dataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mProteinLigandDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataItem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProteinLigandDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mvalid_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProteinLigandDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 3 required positional arguments: 'pairs', 'train_processor', and 'batch_size'"
     ]
    }
   ],
   "source": [
    "# load generated data from disk\n",
    "from src.gen_dataset import ProteinLigandDataset\n",
    "\n",
    "train_set = ProteinLigandDataset()\n",
    "valid_set = ProteinLigandDataset()\n",
    "\n",
    "for i in range(total_pairs_n * train_ratio // chunk_size + 1):\n",
    "    s = torch.load('{}/{}.data'.format(train_save_dir, i))\n",
    "    train_set.concat(s)\n",
    "\n",
    "for j in range((total_pairs_n - valid_ratio) // chunk_size + 1):\n",
    "    s = torch.load('{}/{}.data'.format(valid_save_dir, j))\n",
    "    valid_set.concat(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d7f4da-922c-473c-a463-caae0f07c0e4",
   "metadata": {},
   "source": [
    "### Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024e4946-4cb9-4264-90f4-aa3214ed7406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the train loss accumulates loss from each 1024 vector\n",
    "def train_loss(pred, target, loss_fn):\n",
    "    target = target.expand(-1, pred.shape[1])\n",
    "    loss = loss_fn(pred, target)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3ae3ef-7d54-49f4-aa40-da01a25ceff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_loss(pred, target, loss_fn):\n",
    "    pred = torch.mean(pred, pred.shape[1])\n",
    "    loss = loss_fn(pred, target)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dc0f40-e814-4175-94dd-dc7e491cc411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the training function\n",
    "def train_one_epoch(epoch_index, batch_size, tb_writer, loader, model, loss_fn, optimizer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "    \n",
    "    for batch, X in enumerate(loader):\n",
    "        x1 = X[0]  # grid\n",
    "        x2 = X[1]  # embeds\n",
    "        target = X[2]  # labels\n",
    "        # Compute prediction and loss\n",
    "        pred = model(x1, x2)\n",
    "        loss = train_loss(pred, target, loss_fn)\n",
    "        \n",
    "        # Back Propagate\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Gather data and report to tensorboard\n",
    "        running_loss += loss.item()\n",
    "        if i % batch_size == batch_size - 1:\n",
    "            last_loss = running_loss / batch_size # loss per batch\n",
    "            tb_x = epoch_index * len(training_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "        \n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ade3385-67b9-4129-9b63-7261328c16a3",
   "metadata": {},
   "source": [
    "## Training\n",
    "With everything set, we can start to train the model now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12caff61-66b4-40c1-bb57-1f407c26f2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.assembly_model import AssemblyModel\n",
    "\n",
    "# training hyperparams\n",
    "model_save_dir = '../models'\n",
    "tb_save_dir = '../models/runs'\n",
    "\n",
    "EPOCHS = 50\n",
    "best_loss = 1_000_000\n",
    "batch_size = 4\n",
    "learning_rate = 0.001\n",
    "early_stop_t = 0.1  # threshold for ealy stopping\n",
    "\n",
    "# create model\n",
    "model = AssemblyModel(DEVICE)\n",
    "\n",
    "# utilities\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e390719f-cda7-4f2c-8dce-8199f15bfe47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloaders\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=K_CORES//2)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=K_CORES//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770b9250-d6ed-4a0d-9b5a-03f6f1992869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, let's train!\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('{}/train_{}'.format(tb_save_dir, timestamp))\n",
    "\n",
    "epoch_number = 0\n",
    "\n",
    "with tqdm(total=EPOCHS) as pbar:\n",
    "    for epoch in range(EPOCHS):\n",
    "        pbar.write('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "        # Make sure gradient tracking is on, and do a pass over the data\n",
    "        model.train(True)\n",
    "        avg_loss = train_one_epoch(epoch_number, batch_size, writer, train_loader, model, loss_fn, optimizer)\n",
    "\n",
    "        # We don't need gradients on to do reporting\n",
    "        model.train(False)\n",
    "\n",
    "        running_vloss = 0.0\n",
    "        for i, vdata in enumerate(valid_loader):\n",
    "            vx1, vx2, vlabels = vdata\n",
    "            voutputs = model(vx1, vx2)\n",
    "            vloss = loss_fn(voutputs, vlabels)\n",
    "            running_vloss += vloss\n",
    "\n",
    "        avg_vloss = running_vloss / (i + 1)\n",
    "        pbar.write('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "        # Log the running loss averaged per batch\n",
    "        # for both training and validation\n",
    "        writer.add_scalars('Training vs. Validation Loss',\n",
    "                        { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                        epoch_number + 1)\n",
    "        writer.flush()\n",
    "\n",
    "        # Track best performance, and save the model's state\n",
    "        if avg_vloss < best_vloss:\n",
    "            best_vloss = avg_vloss\n",
    "            model_path = '{}/model_{}_{}'.format(model_save_dir, timestamp, epoch_number)\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "        epoch_number += 1\n",
    "        pbar.update(epoch_number)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
